{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project2-MultiClass.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "gcGlQXMPGgcN",
        "colab_type": "code",
        "outputId": "d7af9508-d6e4-479f-c08f-6ffd18927643",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f41299f4-1bc1-474f-9d20-6f6893f313fa\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-f41299f4-1bc1-474f-9d20-6f6893f313fa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving iris.csv to iris.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7ytQ7EVQil8",
        "colab_type": "code",
        "outputId": "e0de4036-b641-4e5d-c927-ff632198aaad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.utils import np_utils\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tBknhTXiYBcX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seed = 7\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FD0fAvm6YMyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataframe = pd.read_csv(\"iris.csv\", header=None)\n",
        "dataset = dataframe.values\n",
        "X = dataset[:,0:4].astype(float)\n",
        "Y = dataset[:,4]\n",
        "le = LabelEncoder()\n",
        "Y=le.fit_transform(dataset[:,4])\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "encoder.fit(Y)\n",
        "encoded_Y = encoder.transform(Y)\n",
        "# convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = np_utils.to_categorical(encoded_Y)\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, shuffle= True)\n",
        "\n",
        "one_hot_train_labels=to_categorical(y_train)\n",
        "one_hot_test_labels=to_categorical(y_test)\n",
        "\n",
        "x_val=x_train[:25]\n",
        "partial_x_train=x_train[25:]\n",
        "\n",
        "y_val=one_hot_train_labels[:25]\n",
        "partial_y_train=one_hot_train_labels[25:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fJqyPqvbF6E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_baseline():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPz287iORjgN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator = KerasClassifier(build_fn=create_baseline, epochs=200, batch_size=5, verbose=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lByTimCzRma1",
        "colab_type": "code",
        "outputId": "e9fd3d2f-c736-4187-91c6-9d8581780c4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "Baseline: 97.33% (3.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRyuqO49RqaU",
        "colab_type": "code",
        "outputId": "422cc664-1daa-4889-9868-ef878fc599b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def create_small():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(4,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model\n",
        "estimator = KerasClassifier(build_fn=create_small, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 88.00% (25.26%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cci8NlD3WvAT",
        "colab_type": "code",
        "outputId": "54686f0b-f3ac-4af9-be7b-9515499e3e90",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def create_large():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dense(8,activation='relu'))\n",
        "    model.add(Dense(8,activation='relu'))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model\n",
        "estimator = KerasClassifier(build_fn=create_large, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 96.00% (3.27%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5hB6nLVZP2N",
        "colab_type": "code",
        "outputId": "e5e11121-a90e-4726-c1ec-bdab09088bd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def create_overfit():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(64,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dense(64,activation='relu'))\n",
        "    model.add(Dense(64,activation='relu'))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model\n",
        "estimator = KerasClassifier(build_fn=create_overfit, epochs=300, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 96.67% (5.37%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAxEp0kuZX2r",
        "colab_type": "code",
        "outputId": "016ed6e9-cfa6-45dd-f63c-867484651312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def create_small():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model\n",
        "estimator = KerasClassifier(build_fn=create_small, epochs=200, batch_size=5, verbose=0)\n",
        "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
        "results = cross_val_score(estimator, X, dummy_y, cv=kfold)\n",
        "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Baseline: 97.33% (4.42%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fny2IJkzZYoV",
        "colab_type": "code",
        "outputId": "ae9fd7a1-2556-4f70-8757-0bc53c7fcaa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import keras\n",
        "inputs= keras.Input(shape=(4,))\n",
        "x=Dense(8,activation='relu')(inputs)\n",
        "x=Dense(8,activation='relu')(x)\n",
        "outputs=Dense(3,activation='softmax')(x) \n",
        "model=keras.Model(inputs,outputs)\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])    \n",
        "model.fit(partial_x_train,partial_y_train,epochs=100,validation_data=(x_val,y_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 75 samples, validate on 25 samples\n",
            "Epoch 1/100\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.2515 - acc: 0.3333 - val_loss: 1.2126 - val_acc: 0.2800\n",
            "Epoch 2/100\n",
            "75/75 [==============================] - 0s 327us/step - loss: 1.2067 - acc: 0.3333 - val_loss: 1.1736 - val_acc: 0.2800\n",
            "Epoch 3/100\n",
            "75/75 [==============================] - 0s 301us/step - loss: 1.1751 - acc: 0.3333 - val_loss: 1.1516 - val_acc: 0.2800\n",
            "Epoch 4/100\n",
            "75/75 [==============================] - 0s 315us/step - loss: 1.1573 - acc: 0.3333 - val_loss: 1.1375 - val_acc: 0.2800\n",
            "Epoch 5/100\n",
            "75/75 [==============================] - 0s 321us/step - loss: 1.1442 - acc: 0.3333 - val_loss: 1.1188 - val_acc: 0.2800\n",
            "Epoch 6/100\n",
            "75/75 [==============================] - 0s 428us/step - loss: 1.1280 - acc: 0.3333 - val_loss: 1.1051 - val_acc: 0.2800\n",
            "Epoch 7/100\n",
            "75/75 [==============================] - 0s 367us/step - loss: 1.1147 - acc: 0.3333 - val_loss: 1.0927 - val_acc: 0.2800\n",
            "Epoch 8/100\n",
            "75/75 [==============================] - 0s 370us/step - loss: 1.1030 - acc: 0.3733 - val_loss: 1.0829 - val_acc: 0.3600\n",
            "Epoch 9/100\n",
            "75/75 [==============================] - 0s 310us/step - loss: 1.0910 - acc: 0.4133 - val_loss: 1.0706 - val_acc: 0.4400\n",
            "Epoch 10/100\n",
            "75/75 [==============================] - 0s 264us/step - loss: 1.0782 - acc: 0.5200 - val_loss: 1.0621 - val_acc: 0.6400\n",
            "Epoch 11/100\n",
            "75/75 [==============================] - 0s 303us/step - loss: 1.0709 - acc: 0.5733 - val_loss: 1.0566 - val_acc: 0.6800\n",
            "Epoch 12/100\n",
            "75/75 [==============================] - 0s 339us/step - loss: 1.0652 - acc: 0.6400 - val_loss: 1.0514 - val_acc: 0.6800\n",
            "Epoch 13/100\n",
            "75/75 [==============================] - 0s 360us/step - loss: 1.0599 - acc: 0.6533 - val_loss: 1.0450 - val_acc: 0.7200\n",
            "Epoch 14/100\n",
            "75/75 [==============================] - 0s 281us/step - loss: 1.0532 - acc: 0.6667 - val_loss: 1.0388 - val_acc: 0.7200\n",
            "Epoch 15/100\n",
            "75/75 [==============================] - 0s 367us/step - loss: 1.0479 - acc: 0.6667 - val_loss: 1.0346 - val_acc: 0.7200\n",
            "Epoch 16/100\n",
            "75/75 [==============================] - 0s 300us/step - loss: 1.0436 - acc: 0.6667 - val_loss: 1.0300 - val_acc: 0.7200\n",
            "Epoch 17/100\n",
            "75/75 [==============================] - 0s 342us/step - loss: 1.0389 - acc: 0.6667 - val_loss: 1.0247 - val_acc: 0.7200\n",
            "Epoch 18/100\n",
            "75/75 [==============================] - 0s 278us/step - loss: 1.0333 - acc: 0.6667 - val_loss: 1.0202 - val_acc: 0.7200\n",
            "Epoch 19/100\n",
            "75/75 [==============================] - 0s 314us/step - loss: 1.0306 - acc: 0.6667 - val_loss: 1.0157 - val_acc: 0.7200\n",
            "Epoch 20/100\n",
            "75/75 [==============================] - 0s 334us/step - loss: 1.0251 - acc: 0.6667 - val_loss: 1.0122 - val_acc: 0.7200\n",
            "Epoch 21/100\n",
            "75/75 [==============================] - 0s 310us/step - loss: 1.0214 - acc: 0.6667 - val_loss: 1.0080 - val_acc: 0.7200\n",
            "Epoch 22/100\n",
            "75/75 [==============================] - 0s 346us/step - loss: 1.0175 - acc: 0.6667 - val_loss: 1.0048 - val_acc: 0.7200\n",
            "Epoch 23/100\n",
            "75/75 [==============================] - 0s 273us/step - loss: 1.0147 - acc: 0.6667 - val_loss: 1.0023 - val_acc: 0.7200\n",
            "Epoch 24/100\n",
            "75/75 [==============================] - 0s 300us/step - loss: 1.0114 - acc: 0.6667 - val_loss: 0.9991 - val_acc: 0.7200\n",
            "Epoch 25/100\n",
            "75/75 [==============================] - 0s 368us/step - loss: 1.0080 - acc: 0.6667 - val_loss: 0.9955 - val_acc: 0.7200\n",
            "Epoch 26/100\n",
            "75/75 [==============================] - 0s 381us/step - loss: 1.0045 - acc: 0.6667 - val_loss: 0.9919 - val_acc: 0.7200\n",
            "Epoch 27/100\n",
            "75/75 [==============================] - 0s 321us/step - loss: 1.0008 - acc: 0.6667 - val_loss: 0.9887 - val_acc: 0.7200\n",
            "Epoch 28/100\n",
            "75/75 [==============================] - 0s 284us/step - loss: 0.9969 - acc: 0.6667 - val_loss: 0.9846 - val_acc: 0.7200\n",
            "Epoch 29/100\n",
            "75/75 [==============================] - 0s 263us/step - loss: 0.9934 - acc: 0.6667 - val_loss: 0.9805 - val_acc: 0.7200\n",
            "Epoch 30/100\n",
            "75/75 [==============================] - 0s 313us/step - loss: 0.9887 - acc: 0.6667 - val_loss: 0.9765 - val_acc: 0.7200\n",
            "Epoch 31/100\n",
            "75/75 [==============================] - 0s 248us/step - loss: 0.9846 - acc: 0.6667 - val_loss: 0.9725 - val_acc: 0.7200\n",
            "Epoch 32/100\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.9802 - acc: 0.6667 - val_loss: 0.9683 - val_acc: 0.7200\n",
            "Epoch 33/100\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.9758 - acc: 0.6667 - val_loss: 0.9636 - val_acc: 0.7200\n",
            "Epoch 34/100\n",
            "75/75 [==============================] - 0s 373us/step - loss: 0.9717 - acc: 0.6667 - val_loss: 0.9601 - val_acc: 0.7200\n",
            "Epoch 35/100\n",
            "75/75 [==============================] - 0s 326us/step - loss: 0.9677 - acc: 0.6667 - val_loss: 0.9554 - val_acc: 0.7200\n",
            "Epoch 36/100\n",
            "75/75 [==============================] - 0s 305us/step - loss: 0.9631 - acc: 0.6667 - val_loss: 0.9511 - val_acc: 0.7200\n",
            "Epoch 37/100\n",
            "75/75 [==============================] - 0s 311us/step - loss: 0.9589 - acc: 0.6667 - val_loss: 0.9474 - val_acc: 0.7200\n",
            "Epoch 38/100\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.9552 - acc: 0.6667 - val_loss: 0.9426 - val_acc: 0.7200\n",
            "Epoch 39/100\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.9504 - acc: 0.6667 - val_loss: 0.9380 - val_acc: 0.7200\n",
            "Epoch 40/100\n",
            "75/75 [==============================] - 0s 278us/step - loss: 0.9462 - acc: 0.6667 - val_loss: 0.9338 - val_acc: 0.7200\n",
            "Epoch 41/100\n",
            "75/75 [==============================] - 0s 304us/step - loss: 0.9420 - acc: 0.6667 - val_loss: 0.9294 - val_acc: 0.7200\n",
            "Epoch 42/100\n",
            "75/75 [==============================] - 0s 331us/step - loss: 0.9371 - acc: 0.6667 - val_loss: 0.9245 - val_acc: 0.7200\n",
            "Epoch 43/100\n",
            "75/75 [==============================] - 0s 344us/step - loss: 0.9325 - acc: 0.6667 - val_loss: 0.9199 - val_acc: 0.7200\n",
            "Epoch 44/100\n",
            "75/75 [==============================] - 0s 360us/step - loss: 0.9283 - acc: 0.6667 - val_loss: 0.9154 - val_acc: 0.7200\n",
            "Epoch 45/100\n",
            "75/75 [==============================] - 0s 433us/step - loss: 0.9233 - acc: 0.6667 - val_loss: 0.9104 - val_acc: 0.7200\n",
            "Epoch 46/100\n",
            "75/75 [==============================] - 0s 345us/step - loss: 0.9185 - acc: 0.6667 - val_loss: 0.9057 - val_acc: 0.7200\n",
            "Epoch 47/100\n",
            "75/75 [==============================] - 0s 321us/step - loss: 0.9135 - acc: 0.6667 - val_loss: 0.9006 - val_acc: 0.7200\n",
            "Epoch 48/100\n",
            "75/75 [==============================] - 0s 256us/step - loss: 0.9085 - acc: 0.6667 - val_loss: 0.8954 - val_acc: 0.7200\n",
            "Epoch 49/100\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.9035 - acc: 0.6667 - val_loss: 0.8909 - val_acc: 0.7200\n",
            "Epoch 50/100\n",
            "75/75 [==============================] - 0s 309us/step - loss: 0.8987 - acc: 0.6667 - val_loss: 0.8857 - val_acc: 0.7200\n",
            "Epoch 51/100\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.8932 - acc: 0.6667 - val_loss: 0.8805 - val_acc: 0.7200\n",
            "Epoch 52/100\n",
            "75/75 [==============================] - 0s 249us/step - loss: 0.8881 - acc: 0.6667 - val_loss: 0.8748 - val_acc: 0.7200\n",
            "Epoch 53/100\n",
            "75/75 [==============================] - 0s 346us/step - loss: 0.8830 - acc: 0.6667 - val_loss: 0.8693 - val_acc: 0.7200\n",
            "Epoch 54/100\n",
            "75/75 [==============================] - 0s 322us/step - loss: 0.8776 - acc: 0.6667 - val_loss: 0.8644 - val_acc: 0.7200\n",
            "Epoch 55/100\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.8729 - acc: 0.6667 - val_loss: 0.8594 - val_acc: 0.7200\n",
            "Epoch 56/100\n",
            "75/75 [==============================] - 0s 327us/step - loss: 0.8683 - acc: 0.6667 - val_loss: 0.8546 - val_acc: 0.7200\n",
            "Epoch 57/100\n",
            "75/75 [==============================] - 0s 326us/step - loss: 0.8635 - acc: 0.6667 - val_loss: 0.8485 - val_acc: 0.7200\n",
            "Epoch 58/100\n",
            "75/75 [==============================] - 0s 325us/step - loss: 0.8582 - acc: 0.6667 - val_loss: 0.8427 - val_acc: 0.7200\n",
            "Epoch 59/100\n",
            "75/75 [==============================] - 0s 332us/step - loss: 0.8528 - acc: 0.6667 - val_loss: 0.8374 - val_acc: 0.7200\n",
            "Epoch 60/100\n",
            "75/75 [==============================] - 0s 315us/step - loss: 0.8479 - acc: 0.6667 - val_loss: 0.8328 - val_acc: 0.7200\n",
            "Epoch 61/100\n",
            "75/75 [==============================] - 0s 305us/step - loss: 0.8431 - acc: 0.6667 - val_loss: 0.8270 - val_acc: 0.7200\n",
            "Epoch 62/100\n",
            "75/75 [==============================] - 0s 366us/step - loss: 0.8382 - acc: 0.6667 - val_loss: 0.8218 - val_acc: 0.7200\n",
            "Epoch 63/100\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.8335 - acc: 0.6667 - val_loss: 0.8166 - val_acc: 0.7200\n",
            "Epoch 64/100\n",
            "75/75 [==============================] - 0s 262us/step - loss: 0.8291 - acc: 0.6667 - val_loss: 0.8117 - val_acc: 0.7200\n",
            "Epoch 65/100\n",
            "75/75 [==============================] - 0s 300us/step - loss: 0.8241 - acc: 0.6667 - val_loss: 0.8066 - val_acc: 0.7200\n",
            "Epoch 66/100\n",
            "75/75 [==============================] - 0s 373us/step - loss: 0.8194 - acc: 0.6533 - val_loss: 0.8012 - val_acc: 0.7200\n",
            "Epoch 67/100\n",
            "75/75 [==============================] - 0s 304us/step - loss: 0.8146 - acc: 0.6533 - val_loss: 0.7958 - val_acc: 0.7200\n",
            "Epoch 68/100\n",
            "75/75 [==============================] - 0s 297us/step - loss: 0.8112 - acc: 0.6533 - val_loss: 0.7919 - val_acc: 0.7200\n",
            "Epoch 69/100\n",
            "75/75 [==============================] - 0s 321us/step - loss: 0.8067 - acc: 0.6533 - val_loss: 0.7882 - val_acc: 0.7200\n",
            "Epoch 70/100\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.8035 - acc: 0.6533 - val_loss: 0.7843 - val_acc: 0.7200\n",
            "Epoch 71/100\n",
            "75/75 [==============================] - 0s 376us/step - loss: 0.7993 - acc: 0.6533 - val_loss: 0.7793 - val_acc: 0.7200\n",
            "Epoch 72/100\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.7948 - acc: 0.6533 - val_loss: 0.7742 - val_acc: 0.7200\n",
            "Epoch 73/100\n",
            "75/75 [==============================] - 0s 306us/step - loss: 0.7909 - acc: 0.6533 - val_loss: 0.7707 - val_acc: 0.7200\n",
            "Epoch 74/100\n",
            "75/75 [==============================] - 0s 323us/step - loss: 0.7875 - acc: 0.6533 - val_loss: 0.7643 - val_acc: 0.7200\n",
            "Epoch 75/100\n",
            "75/75 [==============================] - 0s 275us/step - loss: 0.7822 - acc: 0.6267 - val_loss: 0.7597 - val_acc: 0.7200\n",
            "Epoch 76/100\n",
            "75/75 [==============================] - 0s 319us/step - loss: 0.7788 - acc: 0.6133 - val_loss: 0.7569 - val_acc: 0.7200\n",
            "Epoch 77/100\n",
            "75/75 [==============================] - 0s 304us/step - loss: 0.7749 - acc: 0.6133 - val_loss: 0.7507 - val_acc: 0.7200\n",
            "Epoch 78/100\n",
            "75/75 [==============================] - 0s 363us/step - loss: 0.7705 - acc: 0.6000 - val_loss: 0.7459 - val_acc: 0.6800\n",
            "Epoch 79/100\n",
            "75/75 [==============================] - 0s 314us/step - loss: 0.7667 - acc: 0.6000 - val_loss: 0.7405 - val_acc: 0.6800\n",
            "Epoch 80/100\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.7623 - acc: 0.5867 - val_loss: 0.7368 - val_acc: 0.6400\n",
            "Epoch 81/100\n",
            "75/75 [==============================] - 0s 337us/step - loss: 0.7583 - acc: 0.5200 - val_loss: 0.7333 - val_acc: 0.6400\n",
            "Epoch 82/100\n",
            "75/75 [==============================] - 0s 314us/step - loss: 0.7549 - acc: 0.5333 - val_loss: 0.7294 - val_acc: 0.6400\n",
            "Epoch 83/100\n",
            "75/75 [==============================] - 0s 259us/step - loss: 0.7508 - acc: 0.5600 - val_loss: 0.7232 - val_acc: 0.6400\n",
            "Epoch 84/100\n",
            "75/75 [==============================] - 0s 347us/step - loss: 0.7468 - acc: 0.4667 - val_loss: 0.7168 - val_acc: 0.5600\n",
            "Epoch 85/100\n",
            "75/75 [==============================] - 0s 393us/step - loss: 0.7419 - acc: 0.4267 - val_loss: 0.7117 - val_acc: 0.5600\n",
            "Epoch 86/100\n",
            "75/75 [==============================] - 0s 334us/step - loss: 0.7379 - acc: 0.4133 - val_loss: 0.7064 - val_acc: 0.5600\n",
            "Epoch 87/100\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.7336 - acc: 0.4000 - val_loss: 0.7003 - val_acc: 0.4400\n",
            "Epoch 88/100\n",
            "75/75 [==============================] - 0s 254us/step - loss: 0.7293 - acc: 0.4267 - val_loss: 0.6957 - val_acc: 0.4400\n",
            "Epoch 89/100\n",
            "75/75 [==============================] - 0s 272us/step - loss: 0.7253 - acc: 0.4667 - val_loss: 0.6903 - val_acc: 0.4400\n",
            "Epoch 90/100\n",
            "75/75 [==============================] - 0s 277us/step - loss: 0.7206 - acc: 0.5467 - val_loss: 0.6863 - val_acc: 0.4400\n",
            "Epoch 91/100\n",
            "75/75 [==============================] - 0s 333us/step - loss: 0.7170 - acc: 0.5333 - val_loss: 0.6822 - val_acc: 0.4400\n",
            "Epoch 92/100\n",
            "75/75 [==============================] - 0s 300us/step - loss: 0.7132 - acc: 0.5067 - val_loss: 0.6762 - val_acc: 0.4400\n",
            "Epoch 93/100\n",
            "75/75 [==============================] - 0s 262us/step - loss: 0.7085 - acc: 0.5333 - val_loss: 0.6711 - val_acc: 0.5200\n",
            "Epoch 94/100\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.7047 - acc: 0.5600 - val_loss: 0.6655 - val_acc: 0.6400\n",
            "Epoch 95/100\n",
            "75/75 [==============================] - 0s 332us/step - loss: 0.7006 - acc: 0.6000 - val_loss: 0.6606 - val_acc: 0.6400\n",
            "Epoch 96/100\n",
            "75/75 [==============================] - 0s 315us/step - loss: 0.6967 - acc: 0.6000 - val_loss: 0.6565 - val_acc: 0.6800\n",
            "Epoch 97/100\n",
            "75/75 [==============================] - 0s 288us/step - loss: 0.6925 - acc: 0.6133 - val_loss: 0.6538 - val_acc: 0.6800\n",
            "Epoch 98/100\n",
            "75/75 [==============================] - 0s 303us/step - loss: 0.6894 - acc: 0.6133 - val_loss: 0.6475 - val_acc: 0.7200\n",
            "Epoch 99/100\n",
            "75/75 [==============================] - 0s 296us/step - loss: 0.6866 - acc: 0.6000 - val_loss: 0.6426 - val_acc: 0.7200\n",
            "Epoch 100/100\n",
            "75/75 [==============================] - 0s 328us/step - loss: 0.6817 - acc: 0.6533 - val_loss: 0.6390 - val_acc: 0.7200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9583517048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIjLqlX2n31c",
        "colab_type": "code",
        "outputId": "a00b7f55-9701-4891-89b6-c86462d8f298",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "class MyModel(keras.Model):\n",
        "    def __init__(self):\n",
        "        super(MyModel,self).__init__()\n",
        "        self.dense1=Dense(8,activation='relu')\n",
        "        self.dense2=Dense(8,activation='relu')\n",
        "        self.dense3=Dense(3,activation='softmax')\n",
        "        \n",
        "    def call(self,inputs):\n",
        "        x=self.dense1(inputs)\n",
        "        x=self.dense2(x)\n",
        "        return self.dense3(x)\n",
        "model=MyModel()\n",
        "model.compile(optimizer='rmsprop',loss='categorical_crossentropy',metrics=['accuracy'])    \n",
        "history=model.fit(partial_x_train,partial_y_train,epochs=200,validation_data=(x_val,y_val))\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 75 samples, validate on 25 samples\n",
            "Epoch 1/200\n",
            "75/75 [==============================] - 4s 54ms/step - loss: 1.2455 - acc: 0.3333 - val_loss: 1.2613 - val_acc: 0.2800\n",
            "Epoch 2/200\n",
            "75/75 [==============================] - 0s 409us/step - loss: 1.2078 - acc: 0.3333 - val_loss: 1.2398 - val_acc: 0.2800\n",
            "Epoch 3/200\n",
            "75/75 [==============================] - 0s 304us/step - loss: 1.1885 - acc: 0.3333 - val_loss: 1.2206 - val_acc: 0.2800\n",
            "Epoch 4/200\n",
            "75/75 [==============================] - 0s 375us/step - loss: 1.1688 - acc: 0.3333 - val_loss: 1.2041 - val_acc: 0.2800\n",
            "Epoch 5/200\n",
            "75/75 [==============================] - 0s 360us/step - loss: 1.1531 - acc: 0.3333 - val_loss: 1.1884 - val_acc: 0.2800\n",
            "Epoch 6/200\n",
            "75/75 [==============================] - 0s 296us/step - loss: 1.1378 - acc: 0.3333 - val_loss: 1.1772 - val_acc: 0.2800\n",
            "Epoch 7/200\n",
            "75/75 [==============================] - 0s 396us/step - loss: 1.1235 - acc: 0.3333 - val_loss: 1.1631 - val_acc: 0.2800\n",
            "Epoch 8/200\n",
            "75/75 [==============================] - 0s 479us/step - loss: 1.1081 - acc: 0.3333 - val_loss: 1.1492 - val_acc: 0.2800\n",
            "Epoch 9/200\n",
            "75/75 [==============================] - 0s 317us/step - loss: 1.0944 - acc: 0.3333 - val_loss: 1.1355 - val_acc: 0.2800\n",
            "Epoch 10/200\n",
            "75/75 [==============================] - 0s 295us/step - loss: 1.0822 - acc: 0.3333 - val_loss: 1.1233 - val_acc: 0.2800\n",
            "Epoch 11/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 1.0691 - acc: 0.3333 - val_loss: 1.1078 - val_acc: 0.2800\n",
            "Epoch 12/200\n",
            "75/75 [==============================] - 0s 360us/step - loss: 1.0569 - acc: 0.3333 - val_loss: 1.0965 - val_acc: 0.2800\n",
            "Epoch 13/200\n",
            "75/75 [==============================] - 0s 303us/step - loss: 1.0448 - acc: 0.3333 - val_loss: 1.0838 - val_acc: 0.2800\n",
            "Epoch 14/200\n",
            "75/75 [==============================] - 0s 361us/step - loss: 1.0340 - acc: 0.3333 - val_loss: 1.0689 - val_acc: 0.2800\n",
            "Epoch 15/200\n",
            "75/75 [==============================] - 0s 317us/step - loss: 1.0217 - acc: 0.3333 - val_loss: 1.0580 - val_acc: 0.2800\n",
            "Epoch 16/200\n",
            "75/75 [==============================] - 0s 261us/step - loss: 1.0109 - acc: 0.3333 - val_loss: 1.0460 - val_acc: 0.2800\n",
            "Epoch 17/200\n",
            "75/75 [==============================] - 0s 296us/step - loss: 0.9992 - acc: 0.3333 - val_loss: 1.0329 - val_acc: 0.2800\n",
            "Epoch 18/200\n",
            "75/75 [==============================] - 0s 290us/step - loss: 0.9875 - acc: 0.3333 - val_loss: 1.0209 - val_acc: 0.2800\n",
            "Epoch 19/200\n",
            "75/75 [==============================] - 0s 305us/step - loss: 0.9766 - acc: 0.3333 - val_loss: 1.0073 - val_acc: 0.2800\n",
            "Epoch 20/200\n",
            "75/75 [==============================] - 0s 317us/step - loss: 0.9659 - acc: 0.3467 - val_loss: 0.9941 - val_acc: 0.3200\n",
            "Epoch 21/200\n",
            "75/75 [==============================] - 0s 291us/step - loss: 0.9549 - acc: 0.3600 - val_loss: 0.9854 - val_acc: 0.3200\n",
            "Epoch 22/200\n",
            "75/75 [==============================] - 0s 317us/step - loss: 0.9441 - acc: 0.3600 - val_loss: 0.9730 - val_acc: 0.4800\n",
            "Epoch 23/200\n",
            "75/75 [==============================] - 0s 376us/step - loss: 0.9333 - acc: 0.5467 - val_loss: 0.9614 - val_acc: 0.5600\n",
            "Epoch 24/200\n",
            "75/75 [==============================] - 0s 284us/step - loss: 0.9225 - acc: 0.6267 - val_loss: 0.9498 - val_acc: 0.5600\n",
            "Epoch 25/200\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.9136 - acc: 0.7333 - val_loss: 0.9393 - val_acc: 0.7200\n",
            "Epoch 26/200\n",
            "75/75 [==============================] - 0s 277us/step - loss: 0.9030 - acc: 0.8000 - val_loss: 0.9283 - val_acc: 0.7600\n",
            "Epoch 27/200\n",
            "75/75 [==============================] - 0s 258us/step - loss: 0.8932 - acc: 0.8533 - val_loss: 0.9174 - val_acc: 0.8000\n",
            "Epoch 28/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.8854 - acc: 0.8667 - val_loss: 0.9075 - val_acc: 0.8000\n",
            "Epoch 29/200\n",
            "75/75 [==============================] - 0s 300us/step - loss: 0.8743 - acc: 0.9067 - val_loss: 0.8973 - val_acc: 0.8000\n",
            "Epoch 30/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.8648 - acc: 0.9067 - val_loss: 0.8847 - val_acc: 0.8000\n",
            "Epoch 31/200\n",
            "75/75 [==============================] - 0s 329us/step - loss: 0.8558 - acc: 0.8933 - val_loss: 0.8769 - val_acc: 0.8000\n",
            "Epoch 32/200\n",
            "75/75 [==============================] - 0s 301us/step - loss: 0.8478 - acc: 0.8800 - val_loss: 0.8644 - val_acc: 0.8400\n",
            "Epoch 33/200\n",
            "75/75 [==============================] - 0s 304us/step - loss: 0.8392 - acc: 0.9467 - val_loss: 0.8548 - val_acc: 0.8000\n",
            "Epoch 34/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.8299 - acc: 0.9067 - val_loss: 0.8454 - val_acc: 0.8400\n",
            "Epoch 35/200\n",
            "75/75 [==============================] - 0s 292us/step - loss: 0.8205 - acc: 0.8800 - val_loss: 0.8344 - val_acc: 0.8400\n",
            "Epoch 36/200\n",
            "75/75 [==============================] - 0s 337us/step - loss: 0.8118 - acc: 0.8667 - val_loss: 0.8245 - val_acc: 0.8800\n",
            "Epoch 37/200\n",
            "75/75 [==============================] - 0s 288us/step - loss: 0.8038 - acc: 0.8933 - val_loss: 0.8143 - val_acc: 0.8800\n",
            "Epoch 38/200\n",
            "75/75 [==============================] - 0s 276us/step - loss: 0.7954 - acc: 0.8667 - val_loss: 0.8047 - val_acc: 0.8400\n",
            "Epoch 39/200\n",
            "75/75 [==============================] - 0s 283us/step - loss: 0.7864 - acc: 0.8667 - val_loss: 0.7956 - val_acc: 0.8000\n",
            "Epoch 40/200\n",
            "75/75 [==============================] - 0s 277us/step - loss: 0.7778 - acc: 0.8267 - val_loss: 0.7856 - val_acc: 0.8000\n",
            "Epoch 41/200\n",
            "75/75 [==============================] - 0s 292us/step - loss: 0.7692 - acc: 0.8133 - val_loss: 0.7731 - val_acc: 0.8400\n",
            "Epoch 42/200\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.7612 - acc: 0.8533 - val_loss: 0.7655 - val_acc: 0.8800\n",
            "Epoch 43/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.7536 - acc: 0.8933 - val_loss: 0.7575 - val_acc: 0.8000\n",
            "Epoch 44/200\n",
            "75/75 [==============================] - 0s 450us/step - loss: 0.7457 - acc: 0.8133 - val_loss: 0.7496 - val_acc: 0.8000\n",
            "Epoch 45/200\n",
            "75/75 [==============================] - 0s 282us/step - loss: 0.7389 - acc: 0.8000 - val_loss: 0.7422 - val_acc: 0.8000\n",
            "Epoch 46/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.7324 - acc: 0.8400 - val_loss: 0.7369 - val_acc: 0.8000\n",
            "Epoch 47/200\n",
            "75/75 [==============================] - 0s 268us/step - loss: 0.7263 - acc: 0.7467 - val_loss: 0.7256 - val_acc: 0.8000\n",
            "Epoch 48/200\n",
            "75/75 [==============================] - 0s 314us/step - loss: 0.7189 - acc: 0.8400 - val_loss: 0.7192 - val_acc: 0.8000\n",
            "Epoch 49/200\n",
            "75/75 [==============================] - 0s 308us/step - loss: 0.7134 - acc: 0.8133 - val_loss: 0.7130 - val_acc: 0.8000\n",
            "Epoch 50/200\n",
            "75/75 [==============================] - 0s 332us/step - loss: 0.7076 - acc: 0.8800 - val_loss: 0.7057 - val_acc: 0.8000\n",
            "Epoch 51/200\n",
            "75/75 [==============================] - 0s 339us/step - loss: 0.7018 - acc: 0.7867 - val_loss: 0.6986 - val_acc: 0.8400\n",
            "Epoch 52/200\n",
            "75/75 [==============================] - 0s 344us/step - loss: 0.6940 - acc: 0.8267 - val_loss: 0.6907 - val_acc: 0.8000\n",
            "Epoch 53/200\n",
            "75/75 [==============================] - 0s 264us/step - loss: 0.6874 - acc: 0.7733 - val_loss: 0.6839 - val_acc: 0.8400\n",
            "Epoch 54/200\n",
            "75/75 [==============================] - 0s 331us/step - loss: 0.6810 - acc: 0.8133 - val_loss: 0.6759 - val_acc: 0.8000\n",
            "Epoch 55/200\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.6749 - acc: 0.7600 - val_loss: 0.6671 - val_acc: 0.8400\n",
            "Epoch 56/200\n",
            "75/75 [==============================] - 0s 273us/step - loss: 0.6679 - acc: 0.8533 - val_loss: 0.6591 - val_acc: 0.8000\n",
            "Epoch 57/200\n",
            "75/75 [==============================] - 0s 279us/step - loss: 0.6605 - acc: 0.7733 - val_loss: 0.6518 - val_acc: 0.8400\n",
            "Epoch 58/200\n",
            "75/75 [==============================] - 0s 350us/step - loss: 0.6540 - acc: 0.8000 - val_loss: 0.6439 - val_acc: 0.8000\n",
            "Epoch 59/200\n",
            "75/75 [==============================] - 0s 313us/step - loss: 0.6478 - acc: 0.7733 - val_loss: 0.6360 - val_acc: 0.8400\n",
            "Epoch 60/200\n",
            "75/75 [==============================] - 0s 336us/step - loss: 0.6418 - acc: 0.8533 - val_loss: 0.6295 - val_acc: 0.8400\n",
            "Epoch 61/200\n",
            "75/75 [==============================] - 0s 334us/step - loss: 0.6360 - acc: 0.8133 - val_loss: 0.6226 - val_acc: 0.8400\n",
            "Epoch 62/200\n",
            "75/75 [==============================] - 0s 324us/step - loss: 0.6283 - acc: 0.8133 - val_loss: 0.6144 - val_acc: 0.8400\n",
            "Epoch 63/200\n",
            "75/75 [==============================] - 0s 272us/step - loss: 0.6219 - acc: 0.8133 - val_loss: 0.6077 - val_acc: 0.8000\n",
            "Epoch 64/200\n",
            "75/75 [==============================] - 0s 381us/step - loss: 0.6155 - acc: 0.7867 - val_loss: 0.6024 - val_acc: 0.7600\n",
            "Epoch 65/200\n",
            "75/75 [==============================] - 0s 296us/step - loss: 0.6107 - acc: 0.7600 - val_loss: 0.5952 - val_acc: 0.8000\n",
            "Epoch 66/200\n",
            "75/75 [==============================] - 0s 350us/step - loss: 0.6048 - acc: 0.7867 - val_loss: 0.5868 - val_acc: 0.8000\n",
            "Epoch 67/200\n",
            "75/75 [==============================] - 0s 340us/step - loss: 0.5972 - acc: 0.8267 - val_loss: 0.5795 - val_acc: 0.8000\n",
            "Epoch 68/200\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.5915 - acc: 0.7600 - val_loss: 0.5736 - val_acc: 0.8400\n",
            "Epoch 69/200\n",
            "75/75 [==============================] - 0s 248us/step - loss: 0.5867 - acc: 0.8267 - val_loss: 0.5681 - val_acc: 0.8800\n",
            "Epoch 70/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.5809 - acc: 0.8533 - val_loss: 0.5621 - val_acc: 0.8800\n",
            "Epoch 71/200\n",
            "75/75 [==============================] - 0s 285us/step - loss: 0.5756 - acc: 0.8800 - val_loss: 0.5566 - val_acc: 0.8800\n",
            "Epoch 72/200\n",
            "75/75 [==============================] - 0s 293us/step - loss: 0.5699 - acc: 0.8800 - val_loss: 0.5498 - val_acc: 0.8800\n",
            "Epoch 73/200\n",
            "75/75 [==============================] - 0s 290us/step - loss: 0.5646 - acc: 0.8800 - val_loss: 0.5434 - val_acc: 0.8800\n",
            "Epoch 74/200\n",
            "75/75 [==============================] - 0s 307us/step - loss: 0.5599 - acc: 0.8800 - val_loss: 0.5389 - val_acc: 0.8400\n",
            "Epoch 75/200\n",
            "75/75 [==============================] - 0s 293us/step - loss: 0.5551 - acc: 0.8133 - val_loss: 0.5336 - val_acc: 0.8000\n",
            "Epoch 76/200\n",
            "75/75 [==============================] - 0s 277us/step - loss: 0.5507 - acc: 0.8267 - val_loss: 0.5291 - val_acc: 0.7600\n",
            "Epoch 77/200\n",
            "75/75 [==============================] - 0s 280us/step - loss: 0.5463 - acc: 0.7600 - val_loss: 0.5225 - val_acc: 0.8400\n",
            "Epoch 78/200\n",
            "75/75 [==============================] - 0s 290us/step - loss: 0.5413 - acc: 0.8400 - val_loss: 0.5176 - val_acc: 0.8400\n",
            "Epoch 79/200\n",
            "75/75 [==============================] - 0s 310us/step - loss: 0.5377 - acc: 0.8400 - val_loss: 0.5133 - val_acc: 0.8000\n",
            "Epoch 80/200\n",
            "75/75 [==============================] - 0s 283us/step - loss: 0.5326 - acc: 0.8400 - val_loss: 0.5068 - val_acc: 0.8400\n",
            "Epoch 81/200\n",
            "75/75 [==============================] - 0s 276us/step - loss: 0.5275 - acc: 0.8400 - val_loss: 0.5027 - val_acc: 0.8400\n",
            "Epoch 82/200\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.5231 - acc: 0.8667 - val_loss: 0.4968 - val_acc: 0.8800\n",
            "Epoch 83/200\n",
            "75/75 [==============================] - 0s 318us/step - loss: 0.5191 - acc: 0.8667 - val_loss: 0.4919 - val_acc: 0.8800\n",
            "Epoch 84/200\n",
            "75/75 [==============================] - 0s 315us/step - loss: 0.5140 - acc: 0.8800 - val_loss: 0.4878 - val_acc: 0.8800\n",
            "Epoch 85/200\n",
            "75/75 [==============================] - 0s 481us/step - loss: 0.5100 - acc: 0.8800 - val_loss: 0.4824 - val_acc: 0.8800\n",
            "Epoch 86/200\n",
            "75/75 [==============================] - 0s 325us/step - loss: 0.5047 - acc: 0.8800 - val_loss: 0.4779 - val_acc: 0.9200\n",
            "Epoch 87/200\n",
            "75/75 [==============================] - 0s 322us/step - loss: 0.5026 - acc: 0.9067 - val_loss: 0.4739 - val_acc: 0.8800\n",
            "Epoch 88/200\n",
            "75/75 [==============================] - 0s 282us/step - loss: 0.4966 - acc: 0.8800 - val_loss: 0.4695 - val_acc: 0.9200\n",
            "Epoch 89/200\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.4927 - acc: 0.8933 - val_loss: 0.4655 - val_acc: 0.9600\n",
            "Epoch 90/200\n",
            "75/75 [==============================] - 0s 292us/step - loss: 0.4883 - acc: 0.9200 - val_loss: 0.4598 - val_acc: 0.9200\n",
            "Epoch 91/200\n",
            "75/75 [==============================] - 0s 336us/step - loss: 0.4835 - acc: 0.9200 - val_loss: 0.4560 - val_acc: 0.8800\n",
            "Epoch 92/200\n",
            "75/75 [==============================] - 0s 278us/step - loss: 0.4801 - acc: 0.8800 - val_loss: 0.4513 - val_acc: 0.9600\n",
            "Epoch 93/200\n",
            "75/75 [==============================] - 0s 330us/step - loss: 0.4760 - acc: 0.9200 - val_loss: 0.4474 - val_acc: 0.8800\n",
            "Epoch 94/200\n",
            "75/75 [==============================] - 0s 341us/step - loss: 0.4724 - acc: 0.8933 - val_loss: 0.4442 - val_acc: 0.8800\n",
            "Epoch 95/200\n",
            "75/75 [==============================] - 0s 304us/step - loss: 0.4683 - acc: 0.8800 - val_loss: 0.4401 - val_acc: 0.9600\n",
            "Epoch 96/200\n",
            "75/75 [==============================] - 0s 284us/step - loss: 0.4662 - acc: 0.9067 - val_loss: 0.4366 - val_acc: 0.9600\n",
            "Epoch 97/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.4609 - acc: 0.8933 - val_loss: 0.4320 - val_acc: 0.9600\n",
            "Epoch 98/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.4573 - acc: 0.9333 - val_loss: 0.4283 - val_acc: 0.9200\n",
            "Epoch 99/200\n",
            "75/75 [==============================] - 0s 302us/step - loss: 0.4535 - acc: 0.9200 - val_loss: 0.4247 - val_acc: 0.9200\n",
            "Epoch 100/200\n",
            "75/75 [==============================] - 0s 338us/step - loss: 0.4506 - acc: 0.8933 - val_loss: 0.4211 - val_acc: 0.8800\n",
            "Epoch 101/200\n",
            "75/75 [==============================] - 0s 312us/step - loss: 0.4487 - acc: 0.9067 - val_loss: 0.4179 - val_acc: 0.8800\n",
            "Epoch 102/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.4447 - acc: 0.8800 - val_loss: 0.4144 - val_acc: 0.8800\n",
            "Epoch 103/200\n",
            "75/75 [==============================] - 0s 335us/step - loss: 0.4415 - acc: 0.8800 - val_loss: 0.4091 - val_acc: 0.9600\n",
            "Epoch 104/200\n",
            "75/75 [==============================] - 0s 293us/step - loss: 0.4376 - acc: 0.9067 - val_loss: 0.4058 - val_acc: 0.9600\n",
            "Epoch 105/200\n",
            "75/75 [==============================] - 0s 293us/step - loss: 0.4348 - acc: 0.9333 - val_loss: 0.4028 - val_acc: 0.9600\n",
            "Epoch 106/200\n",
            "75/75 [==============================] - 0s 297us/step - loss: 0.4314 - acc: 0.9333 - val_loss: 0.3998 - val_acc: 0.9600\n",
            "Epoch 107/200\n",
            "75/75 [==============================] - 0s 316us/step - loss: 0.4279 - acc: 0.9200 - val_loss: 0.3967 - val_acc: 0.9600\n",
            "Epoch 108/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.4243 - acc: 0.9333 - val_loss: 0.3932 - val_acc: 0.9600\n",
            "Epoch 109/200\n",
            "75/75 [==============================] - 0s 332us/step - loss: 0.4215 - acc: 0.9200 - val_loss: 0.3904 - val_acc: 0.9600\n",
            "Epoch 110/200\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.4185 - acc: 0.9467 - val_loss: 0.3873 - val_acc: 0.9600\n",
            "Epoch 111/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.4150 - acc: 0.9333 - val_loss: 0.3840 - val_acc: 0.9600\n",
            "Epoch 112/200\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.4137 - acc: 0.9333 - val_loss: 0.3813 - val_acc: 0.9600\n",
            "Epoch 113/200\n",
            "75/75 [==============================] - 0s 292us/step - loss: 0.4110 - acc: 0.9200 - val_loss: 0.3792 - val_acc: 0.9600\n",
            "Epoch 114/200\n",
            "75/75 [==============================] - 0s 273us/step - loss: 0.4077 - acc: 0.9200 - val_loss: 0.3759 - val_acc: 0.9600\n",
            "Epoch 115/200\n",
            "75/75 [==============================] - 0s 274us/step - loss: 0.4044 - acc: 0.9200 - val_loss: 0.3727 - val_acc: 0.9600\n",
            "Epoch 116/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.4018 - acc: 0.9333 - val_loss: 0.3707 - val_acc: 0.9600\n",
            "Epoch 117/200\n",
            "75/75 [==============================] - 0s 297us/step - loss: 0.3991 - acc: 0.9200 - val_loss: 0.3681 - val_acc: 0.9600\n",
            "Epoch 118/200\n",
            "75/75 [==============================] - 0s 365us/step - loss: 0.3972 - acc: 0.9333 - val_loss: 0.3663 - val_acc: 0.9600\n",
            "Epoch 119/200\n",
            "75/75 [==============================] - 0s 291us/step - loss: 0.3947 - acc: 0.9200 - val_loss: 0.3629 - val_acc: 0.9600\n",
            "Epoch 120/200\n",
            "75/75 [==============================] - 0s 312us/step - loss: 0.3911 - acc: 0.9333 - val_loss: 0.3598 - val_acc: 0.9600\n",
            "Epoch 121/200\n",
            "75/75 [==============================] - 0s 288us/step - loss: 0.3897 - acc: 0.9333 - val_loss: 0.3576 - val_acc: 0.9600\n",
            "Epoch 122/200\n",
            "75/75 [==============================] - 0s 332us/step - loss: 0.3862 - acc: 0.9333 - val_loss: 0.3552 - val_acc: 0.9600\n",
            "Epoch 123/200\n",
            "75/75 [==============================] - 0s 292us/step - loss: 0.3840 - acc: 0.9333 - val_loss: 0.3524 - val_acc: 0.9600\n",
            "Epoch 124/200\n",
            "75/75 [==============================] - 0s 295us/step - loss: 0.3801 - acc: 0.9467 - val_loss: 0.3502 - val_acc: 1.0000\n",
            "Epoch 125/200\n",
            "75/75 [==============================] - 0s 464us/step - loss: 0.3774 - acc: 0.9467 - val_loss: 0.3471 - val_acc: 1.0000\n",
            "Epoch 126/200\n",
            "75/75 [==============================] - 0s 261us/step - loss: 0.3753 - acc: 0.9333 - val_loss: 0.3442 - val_acc: 1.0000\n",
            "Epoch 127/200\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.3747 - acc: 0.9467 - val_loss: 0.3427 - val_acc: 0.9600\n",
            "Epoch 128/200\n",
            "75/75 [==============================] - 0s 268us/step - loss: 0.3705 - acc: 0.9333 - val_loss: 0.3396 - val_acc: 0.9600\n",
            "Epoch 129/200\n",
            "75/75 [==============================] - 0s 290us/step - loss: 0.3689 - acc: 0.9333 - val_loss: 0.3366 - val_acc: 0.9600\n",
            "Epoch 130/200\n",
            "75/75 [==============================] - 0s 279us/step - loss: 0.3660 - acc: 0.9467 - val_loss: 0.3345 - val_acc: 0.9600\n",
            "Epoch 131/200\n",
            "75/75 [==============================] - 0s 334us/step - loss: 0.3626 - acc: 0.9333 - val_loss: 0.3322 - val_acc: 1.0000\n",
            "Epoch 132/200\n",
            "75/75 [==============================] - 0s 319us/step - loss: 0.3608 - acc: 0.9600 - val_loss: 0.3307 - val_acc: 0.9600\n",
            "Epoch 133/200\n",
            "75/75 [==============================] - 0s 275us/step - loss: 0.3593 - acc: 0.9333 - val_loss: 0.3291 - val_acc: 0.9600\n",
            "Epoch 134/200\n",
            "75/75 [==============================] - 0s 356us/step - loss: 0.3580 - acc: 0.9467 - val_loss: 0.3256 - val_acc: 0.9600\n",
            "Epoch 135/200\n",
            "75/75 [==============================] - 0s 312us/step - loss: 0.3555 - acc: 0.9467 - val_loss: 0.3233 - val_acc: 0.9600\n",
            "Epoch 136/200\n",
            "75/75 [==============================] - 0s 297us/step - loss: 0.3520 - acc: 0.9467 - val_loss: 0.3209 - val_acc: 1.0000\n",
            "Epoch 137/200\n",
            "75/75 [==============================] - 0s 274us/step - loss: 0.3491 - acc: 0.9467 - val_loss: 0.3191 - val_acc: 1.0000\n",
            "Epoch 138/200\n",
            "75/75 [==============================] - 0s 312us/step - loss: 0.3469 - acc: 0.9333 - val_loss: 0.3167 - val_acc: 1.0000\n",
            "Epoch 139/200\n",
            "75/75 [==============================] - 0s 302us/step - loss: 0.3445 - acc: 0.9467 - val_loss: 0.3144 - val_acc: 0.9600\n",
            "Epoch 140/200\n",
            "75/75 [==============================] - 0s 303us/step - loss: 0.3423 - acc: 0.9467 - val_loss: 0.3122 - val_acc: 1.0000\n",
            "Epoch 141/200\n",
            "75/75 [==============================] - 0s 348us/step - loss: 0.3407 - acc: 0.9467 - val_loss: 0.3100 - val_acc: 1.0000\n",
            "Epoch 142/200\n",
            "75/75 [==============================] - 0s 334us/step - loss: 0.3377 - acc: 0.9467 - val_loss: 0.3081 - val_acc: 0.9600\n",
            "Epoch 143/200\n",
            "75/75 [==============================] - 0s 309us/step - loss: 0.3368 - acc: 0.9467 - val_loss: 0.3064 - val_acc: 1.0000\n",
            "Epoch 144/200\n",
            "75/75 [==============================] - 0s 311us/step - loss: 0.3334 - acc: 0.9467 - val_loss: 0.3037 - val_acc: 1.0000\n",
            "Epoch 145/200\n",
            "75/75 [==============================] - 0s 314us/step - loss: 0.3305 - acc: 0.9467 - val_loss: 0.3015 - val_acc: 1.0000\n",
            "Epoch 146/200\n",
            "75/75 [==============================] - 0s 284us/step - loss: 0.3287 - acc: 0.9467 - val_loss: 0.2994 - val_acc: 1.0000\n",
            "Epoch 147/200\n",
            "75/75 [==============================] - 0s 290us/step - loss: 0.3264 - acc: 0.9467 - val_loss: 0.2977 - val_acc: 1.0000\n",
            "Epoch 148/200\n",
            "75/75 [==============================] - 0s 271us/step - loss: 0.3243 - acc: 0.9467 - val_loss: 0.2958 - val_acc: 0.9600\n",
            "Epoch 149/200\n",
            "75/75 [==============================] - 0s 326us/step - loss: 0.3214 - acc: 0.9467 - val_loss: 0.2931 - val_acc: 0.9600\n",
            "Epoch 150/200\n",
            "75/75 [==============================] - 0s 288us/step - loss: 0.3220 - acc: 0.9467 - val_loss: 0.2915 - val_acc: 0.9600\n",
            "Epoch 151/200\n",
            "75/75 [==============================] - 0s 354us/step - loss: 0.3176 - acc: 0.9467 - val_loss: 0.2896 - val_acc: 0.9600\n",
            "Epoch 152/200\n",
            "75/75 [==============================] - 0s 432us/step - loss: 0.3188 - acc: 0.9467 - val_loss: 0.2875 - val_acc: 0.9600\n",
            "Epoch 153/200\n",
            "75/75 [==============================] - 0s 272us/step - loss: 0.3136 - acc: 0.9467 - val_loss: 0.2862 - val_acc: 0.9600\n",
            "Epoch 154/200\n",
            "75/75 [==============================] - 0s 275us/step - loss: 0.3114 - acc: 0.9467 - val_loss: 0.2835 - val_acc: 0.9600\n",
            "Epoch 155/200\n",
            "75/75 [==============================] - 0s 297us/step - loss: 0.3127 - acc: 0.9333 - val_loss: 0.2814 - val_acc: 1.0000\n",
            "Epoch 156/200\n",
            "75/75 [==============================] - 0s 265us/step - loss: 0.3078 - acc: 0.9467 - val_loss: 0.2796 - val_acc: 1.0000\n",
            "Epoch 157/200\n",
            "75/75 [==============================] - 0s 270us/step - loss: 0.3059 - acc: 0.9467 - val_loss: 0.2780 - val_acc: 1.0000\n",
            "Epoch 158/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.3052 - acc: 0.9467 - val_loss: 0.2768 - val_acc: 1.0000\n",
            "Epoch 159/200\n",
            "75/75 [==============================] - 0s 320us/step - loss: 0.3054 - acc: 0.9467 - val_loss: 0.2746 - val_acc: 1.0000\n",
            "Epoch 160/200\n",
            "75/75 [==============================] - 0s 299us/step - loss: 0.3007 - acc: 0.9600 - val_loss: 0.2723 - val_acc: 1.0000\n",
            "Epoch 161/200\n",
            "75/75 [==============================] - 0s 322us/step - loss: 0.2991 - acc: 0.9467 - val_loss: 0.2722 - val_acc: 0.9600\n",
            "Epoch 162/200\n",
            "75/75 [==============================] - 0s 273us/step - loss: 0.2966 - acc: 0.9467 - val_loss: 0.2725 - val_acc: 0.9600\n",
            "Epoch 163/200\n",
            "75/75 [==============================] - 0s 381us/step - loss: 0.2956 - acc: 0.9467 - val_loss: 0.2701 - val_acc: 0.9600\n",
            "Epoch 164/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.2933 - acc: 0.9467 - val_loss: 0.2703 - val_acc: 0.9600\n",
            "Epoch 165/200\n",
            "75/75 [==============================] - 0s 489us/step - loss: 0.2921 - acc: 0.9467 - val_loss: 0.2669 - val_acc: 0.9600\n",
            "Epoch 166/200\n",
            "75/75 [==============================] - 0s 326us/step - loss: 0.2888 - acc: 0.9467 - val_loss: 0.2637 - val_acc: 0.9600\n",
            "Epoch 167/200\n",
            "75/75 [==============================] - 0s 279us/step - loss: 0.2870 - acc: 0.9467 - val_loss: 0.2617 - val_acc: 0.9600\n",
            "Epoch 168/200\n",
            "75/75 [==============================] - 0s 294us/step - loss: 0.2857 - acc: 0.9467 - val_loss: 0.2603 - val_acc: 0.9600\n",
            "Epoch 169/200\n",
            "75/75 [==============================] - 0s 355us/step - loss: 0.2847 - acc: 0.9467 - val_loss: 0.2595 - val_acc: 0.9600\n",
            "Epoch 170/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.2834 - acc: 0.9467 - val_loss: 0.2576 - val_acc: 0.9600\n",
            "Epoch 171/200\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.2797 - acc: 0.9467 - val_loss: 0.2542 - val_acc: 1.0000\n",
            "Epoch 172/200\n",
            "75/75 [==============================] - 0s 325us/step - loss: 0.2779 - acc: 0.9467 - val_loss: 0.2524 - val_acc: 1.0000\n",
            "Epoch 173/200\n",
            "75/75 [==============================] - 0s 347us/step - loss: 0.2769 - acc: 0.9467 - val_loss: 0.2508 - val_acc: 1.0000\n",
            "Epoch 174/200\n",
            "75/75 [==============================] - 0s 349us/step - loss: 0.2737 - acc: 0.9467 - val_loss: 0.2503 - val_acc: 0.9600\n",
            "Epoch 175/200\n",
            "75/75 [==============================] - 0s 258us/step - loss: 0.2732 - acc: 0.9467 - val_loss: 0.2514 - val_acc: 0.9600\n",
            "Epoch 176/200\n",
            "75/75 [==============================] - 0s 380us/step - loss: 0.2719 - acc: 0.9467 - val_loss: 0.2465 - val_acc: 0.9600\n",
            "Epoch 177/200\n",
            "75/75 [==============================] - 0s 319us/step - loss: 0.2697 - acc: 0.9467 - val_loss: 0.2468 - val_acc: 0.9600\n",
            "Epoch 178/200\n",
            "75/75 [==============================] - 0s 279us/step - loss: 0.2669 - acc: 0.9333 - val_loss: 0.2441 - val_acc: 0.9600\n",
            "Epoch 179/200\n",
            "75/75 [==============================] - 0s 266us/step - loss: 0.2657 - acc: 0.9467 - val_loss: 0.2424 - val_acc: 0.9600\n",
            "Epoch 180/200\n",
            "75/75 [==============================] - 0s 268us/step - loss: 0.2634 - acc: 0.9467 - val_loss: 0.2400 - val_acc: 1.0000\n",
            "Epoch 181/200\n",
            "75/75 [==============================] - 0s 342us/step - loss: 0.2625 - acc: 0.9467 - val_loss: 0.2387 - val_acc: 1.0000\n",
            "Epoch 182/200\n",
            "75/75 [==============================] - 0s 364us/step - loss: 0.2613 - acc: 0.9467 - val_loss: 0.2367 - val_acc: 1.0000\n",
            "Epoch 183/200\n",
            "75/75 [==============================] - 0s 306us/step - loss: 0.2585 - acc: 0.9467 - val_loss: 0.2363 - val_acc: 0.9600\n",
            "Epoch 184/200\n",
            "75/75 [==============================] - 0s 322us/step - loss: 0.2568 - acc: 0.9467 - val_loss: 0.2363 - val_acc: 0.9600\n",
            "Epoch 185/200\n",
            "75/75 [==============================] - 0s 281us/step - loss: 0.2558 - acc: 0.9333 - val_loss: 0.2367 - val_acc: 0.9600\n",
            "Epoch 186/200\n",
            "75/75 [==============================] - 0s 285us/step - loss: 0.2533 - acc: 0.9467 - val_loss: 0.2328 - val_acc: 0.9600\n",
            "Epoch 187/200\n",
            "75/75 [==============================] - 0s 298us/step - loss: 0.2554 - acc: 0.9467 - val_loss: 0.2351 - val_acc: 0.9600\n",
            "Epoch 188/200\n",
            "75/75 [==============================] - 0s 319us/step - loss: 0.2510 - acc: 0.9467 - val_loss: 0.2329 - val_acc: 0.9600\n",
            "Epoch 189/200\n",
            "75/75 [==============================] - 0s 285us/step - loss: 0.2514 - acc: 0.9333 - val_loss: 0.2338 - val_acc: 0.9600\n",
            "Epoch 190/200\n",
            "75/75 [==============================] - 0s 289us/step - loss: 0.2491 - acc: 0.9467 - val_loss: 0.2307 - val_acc: 0.9600\n",
            "Epoch 191/200\n",
            "75/75 [==============================] - 0s 325us/step - loss: 0.2466 - acc: 0.9333 - val_loss: 0.2274 - val_acc: 0.9600\n",
            "Epoch 192/200\n",
            "75/75 [==============================] - 0s 334us/step - loss: 0.2444 - acc: 0.9600 - val_loss: 0.2244 - val_acc: 0.9600\n",
            "Epoch 193/200\n",
            "75/75 [==============================] - 0s 335us/step - loss: 0.2424 - acc: 0.9467 - val_loss: 0.2218 - val_acc: 0.9600\n",
            "Epoch 194/200\n",
            "75/75 [==============================] - 0s 270us/step - loss: 0.2422 - acc: 0.9467 - val_loss: 0.2216 - val_acc: 0.9600\n",
            "Epoch 195/200\n",
            "75/75 [==============================] - 0s 371us/step - loss: 0.2406 - acc: 0.9467 - val_loss: 0.2243 - val_acc: 0.9600\n",
            "Epoch 196/200\n",
            "75/75 [==============================] - 0s 310us/step - loss: 0.2418 - acc: 0.9467 - val_loss: 0.2193 - val_acc: 0.9600\n",
            "Epoch 197/200\n",
            "75/75 [==============================] - 0s 331us/step - loss: 0.2370 - acc: 0.9467 - val_loss: 0.2179 - val_acc: 0.9600\n",
            "Epoch 198/200\n",
            "75/75 [==============================] - 0s 318us/step - loss: 0.2349 - acc: 0.9467 - val_loss: 0.2167 - val_acc: 0.9600\n",
            "Epoch 199/200\n",
            "75/75 [==============================] - 0s 354us/step - loss: 0.2351 - acc: 0.9467 - val_loss: 0.2186 - val_acc: 0.9600\n",
            "Epoch 200/200\n",
            "75/75 [==============================] - 0s 330us/step - loss: 0.2340 - acc: 0.9600 - val_loss: 0.2135 - val_acc: 0.9600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\naCgopZSq8f8aVCWbpr1cogAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_T2-FVuoCCW",
        "colab_type": "code",
        "outputId": "b6509ff6-ab6c-4eb6-ce7e-b66631cd2dce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "k=4\n",
        "num_val_samples=len(X)//k\n",
        "num_epochs=100\n",
        "all_scores=[]\n",
        "\n",
        "def create_model():\n",
        "    model=Sequential()\n",
        "    model.add(Dense(8,activation='relu',input_shape=(4,)))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(optimizer='Adam',loss='categorical_crossentropy',metrics=['accuracy'])\n",
        "    \n",
        "    #model.fit(partial_x_train,partial_y_train,epochs=5,validation_data=(x_val,y_val))\n",
        "    return model\n",
        "\n",
        "for i in range(k):\n",
        "    print('processing fold #', i)\n",
        "    val_data = X[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    val_targets = dummy_y[i * num_val_samples: (i + 1) * num_val_samples]\n",
        "    partial_train_data = np.concatenate([X[:i * num_val_samples],X[(i + 1) * num_val_samples:]],axis=0)\n",
        "    partial_train_targets = np.concatenate([dummy_y[:i * num_val_samples],dummy_y[(i + 1) * num_val_samples:]],axis=0)\n",
        "    model = create_model()\n",
        "    model.fit(partial_train_data, partial_train_targets,epochs=num_epochs, batch_size=1, verbose=1)\n",
        "    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=1)\n",
        "    all_scores.append(val_mae)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 4s 38ms/step - loss: 1.2265 - acc: 0.4248\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.9153 - acc: 0.5133\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.8572 - acc: 0.5487\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.8416 - acc: 0.6195\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.8147 - acc: 0.6195\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7982 - acc: 0.6372\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7891 - acc: 0.6018\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7733 - acc: 0.6460\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7504 - acc: 0.6637\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7299 - acc: 0.6637\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7221 - acc: 0.6903\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7019 - acc: 0.7168\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6875 - acc: 0.7434\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6758 - acc: 0.6991\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6605 - acc: 0.7080\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6399 - acc: 0.7434\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6286 - acc: 0.7699\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6229 - acc: 0.7080\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6039 - acc: 0.7345\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5767 - acc: 0.7699\n",
            "Epoch 21/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5656 - acc: 0.7345\n",
            "Epoch 22/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5427 - acc: 0.7788\n",
            "Epoch 23/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5264 - acc: 0.7788\n",
            "Epoch 24/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5153 - acc: 0.8496\n",
            "Epoch 25/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4962 - acc: 0.8761\n",
            "Epoch 26/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4961 - acc: 0.8584\n",
            "Epoch 27/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4713 - acc: 0.8761\n",
            "Epoch 28/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4549 - acc: 0.9204\n",
            "Epoch 29/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4545 - acc: 0.8850\n",
            "Epoch 30/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.4398 - acc: 0.9292\n",
            "Epoch 31/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4363 - acc: 0.8761\n",
            "Epoch 32/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4124 - acc: 0.9292\n",
            "Epoch 33/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4038 - acc: 0.9292\n",
            "Epoch 34/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3908 - acc: 0.9292\n",
            "Epoch 35/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3857 - acc: 0.8938\n",
            "Epoch 36/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3684 - acc: 0.9381\n",
            "Epoch 37/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3574 - acc: 0.9469\n",
            "Epoch 38/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3485 - acc: 0.9558\n",
            "Epoch 39/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3364 - acc: 0.9469\n",
            "Epoch 40/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3348 - acc: 0.9204\n",
            "Epoch 41/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3259 - acc: 0.9115\n",
            "Epoch 42/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3151 - acc: 0.9469\n",
            "Epoch 43/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3119 - acc: 0.9381\n",
            "Epoch 44/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3028 - acc: 0.9381\n",
            "Epoch 45/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2938 - acc: 0.9292\n",
            "Epoch 46/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2924 - acc: 0.9292\n",
            "Epoch 47/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2737 - acc: 0.9469\n",
            "Epoch 48/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2711 - acc: 0.9469\n",
            "Epoch 49/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2618 - acc: 0.9469\n",
            "Epoch 50/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2582 - acc: 0.9558\n",
            "Epoch 51/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2656 - acc: 0.9469\n",
            "Epoch 52/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2500 - acc: 0.9558\n",
            "Epoch 53/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2535 - acc: 0.9381\n",
            "Epoch 54/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2269 - acc: 0.9469\n",
            "Epoch 55/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2364 - acc: 0.9558\n",
            "Epoch 56/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2249 - acc: 0.9558\n",
            "Epoch 57/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2186 - acc: 0.9558\n",
            "Epoch 58/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2194 - acc: 0.9558\n",
            "Epoch 59/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2149 - acc: 0.9558\n",
            "Epoch 60/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2074 - acc: 0.9469\n",
            "Epoch 61/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2032 - acc: 0.9558\n",
            "Epoch 62/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1985 - acc: 0.9469\n",
            "Epoch 63/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1977 - acc: 0.9735\n",
            "Epoch 64/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1913 - acc: 0.9646\n",
            "Epoch 65/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1867 - acc: 0.9558\n",
            "Epoch 66/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.1887 - acc: 0.9646\n",
            "Epoch 67/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1838 - acc: 0.9558\n",
            "Epoch 68/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1790 - acc: 0.9558\n",
            "Epoch 69/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1830 - acc: 0.9558\n",
            "Epoch 70/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1734 - acc: 0.9469\n",
            "Epoch 71/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1699 - acc: 0.9646\n",
            "Epoch 72/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1730 - acc: 0.9646\n",
            "Epoch 73/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1618 - acc: 0.9558\n",
            "Epoch 74/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1676 - acc: 0.9646\n",
            "Epoch 75/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1687 - acc: 0.9558\n",
            "Epoch 76/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1592 - acc: 0.9558\n",
            "Epoch 77/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.1572 - acc: 0.9558\n",
            "Epoch 78/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1532 - acc: 0.9735\n",
            "Epoch 79/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1522 - acc: 0.9558\n",
            "Epoch 80/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1521 - acc: 0.9558\n",
            "Epoch 81/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1544 - acc: 0.9646\n",
            "Epoch 82/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1467 - acc: 0.9646\n",
            "Epoch 83/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1448 - acc: 0.9646\n",
            "Epoch 84/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1439 - acc: 0.9558\n",
            "Epoch 85/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1459 - acc: 0.9646\n",
            "Epoch 86/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1431 - acc: 0.9558\n",
            "Epoch 87/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1405 - acc: 0.9558\n",
            "Epoch 88/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1354 - acc: 0.9735\n",
            "Epoch 89/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1356 - acc: 0.9823\n",
            "Epoch 90/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1342 - acc: 0.9823\n",
            "Epoch 91/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1415 - acc: 0.9469\n",
            "Epoch 92/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1354 - acc: 0.9558\n",
            "Epoch 93/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1335 - acc: 0.9558\n",
            "Epoch 94/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1352 - acc: 0.9646\n",
            "Epoch 95/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1280 - acc: 0.9558\n",
            "Epoch 96/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1265 - acc: 0.9735\n",
            "Epoch 97/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1274 - acc: 0.9646\n",
            "Epoch 98/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1327 - acc: 0.9469\n",
            "Epoch 99/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1229 - acc: 0.9646\n",
            "Epoch 100/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1320 - acc: 0.9558\n",
            "37/37 [==============================] - 2s 41ms/step\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 4s 38ms/step - loss: 1.3339 - acc: 0.4248\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 1.0649 - acc: 0.4425\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.9094 - acc: 0.5664\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.7789 - acc: 0.7522\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6823 - acc: 0.7699\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5995 - acc: 0.7699\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5438 - acc: 0.7699\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4952 - acc: 0.7699\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4675 - acc: 0.7699\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4290 - acc: 0.7788\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4146 - acc: 0.7876\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3866 - acc: 0.8230\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3702 - acc: 0.8230\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3579 - acc: 0.8319\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3425 - acc: 0.8673\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3326 - acc: 0.8761\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3223 - acc: 0.8673\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3104 - acc: 0.8938\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3037 - acc: 0.8850\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2918 - acc: 0.9292\n",
            "Epoch 21/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2807 - acc: 0.9115\n",
            "Epoch 22/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2774 - acc: 0.9292\n",
            "Epoch 23/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2714 - acc: 0.9381\n",
            "Epoch 24/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2655 - acc: 0.9381\n",
            "Epoch 25/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2562 - acc: 0.9115\n",
            "Epoch 26/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2430 - acc: 0.9558\n",
            "Epoch 27/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2375 - acc: 0.9381\n",
            "Epoch 28/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2358 - acc: 0.9558\n",
            "Epoch 29/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2289 - acc: 0.9558\n",
            "Epoch 30/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2217 - acc: 0.9204\n",
            "Epoch 31/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2195 - acc: 0.9735\n",
            "Epoch 32/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2102 - acc: 0.9646\n",
            "Epoch 33/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2028 - acc: 0.9735\n",
            "Epoch 34/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2038 - acc: 0.9558\n",
            "Epoch 35/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1956 - acc: 0.9823\n",
            "Epoch 36/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1954 - acc: 0.9646\n",
            "Epoch 37/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1813 - acc: 0.9646\n",
            "Epoch 38/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1849 - acc: 0.9646\n",
            "Epoch 39/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1775 - acc: 0.9646\n",
            "Epoch 40/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1749 - acc: 0.9558\n",
            "Epoch 41/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1706 - acc: 0.9646\n",
            "Epoch 42/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1709 - acc: 0.9646\n",
            "Epoch 43/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1650 - acc: 0.9558\n",
            "Epoch 44/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1576 - acc: 0.9735\n",
            "Epoch 45/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1546 - acc: 0.9735\n",
            "Epoch 46/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1524 - acc: 0.9646\n",
            "Epoch 47/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1544 - acc: 0.9735\n",
            "Epoch 48/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1463 - acc: 0.9735\n",
            "Epoch 49/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1438 - acc: 0.9735\n",
            "Epoch 50/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1421 - acc: 0.9823\n",
            "Epoch 51/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1401 - acc: 0.9735\n",
            "Epoch 52/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1371 - acc: 0.9646\n",
            "Epoch 53/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1317 - acc: 0.9823\n",
            "Epoch 54/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1325 - acc: 0.9735\n",
            "Epoch 55/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1299 - acc: 0.9735\n",
            "Epoch 56/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1313 - acc: 0.9735\n",
            "Epoch 57/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1252 - acc: 0.9823\n",
            "Epoch 58/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1233 - acc: 0.9735\n",
            "Epoch 59/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1215 - acc: 0.9735\n",
            "Epoch 60/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1188 - acc: 0.9823\n",
            "Epoch 61/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1167 - acc: 0.9735\n",
            "Epoch 62/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1194 - acc: 0.9735\n",
            "Epoch 63/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1179 - acc: 0.9558\n",
            "Epoch 64/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1154 - acc: 0.9735\n",
            "Epoch 65/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1132 - acc: 0.9735\n",
            "Epoch 66/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1120 - acc: 0.9735\n",
            "Epoch 67/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1132 - acc: 0.9735\n",
            "Epoch 68/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.1066 - acc: 0.9646\n",
            "Epoch 69/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1081 - acc: 0.9735\n",
            "Epoch 70/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1055 - acc: 0.9646\n",
            "Epoch 71/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1035 - acc: 0.9823\n",
            "Epoch 72/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1008 - acc: 0.9735\n",
            "Epoch 73/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1032 - acc: 0.9823\n",
            "Epoch 74/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1054 - acc: 0.9735\n",
            "Epoch 75/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1021 - acc: 0.9735\n",
            "Epoch 76/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.0995 - acc: 0.9823\n",
            "Epoch 77/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0964 - acc: 0.9823\n",
            "Epoch 78/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0962 - acc: 0.9735\n",
            "Epoch 79/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0956 - acc: 0.9735\n",
            "Epoch 80/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1016 - acc: 0.9646\n",
            "Epoch 81/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0956 - acc: 0.9823\n",
            "Epoch 82/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.0989 - acc: 0.9646\n",
            "Epoch 83/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0888 - acc: 0.9735\n",
            "Epoch 84/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0971 - acc: 0.9646\n",
            "Epoch 85/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0957 - acc: 0.9912\n",
            "Epoch 86/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0962 - acc: 0.9735\n",
            "Epoch 87/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0857 - acc: 0.9912\n",
            "Epoch 88/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.0829 - acc: 0.9823\n",
            "Epoch 89/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0895 - acc: 0.9823\n",
            "Epoch 90/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0898 - acc: 0.9912\n",
            "Epoch 91/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0889 - acc: 0.9735\n",
            "Epoch 92/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0838 - acc: 0.9912\n",
            "Epoch 93/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0956 - acc: 0.9735\n",
            "Epoch 94/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0807 - acc: 0.9823\n",
            "Epoch 95/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.0874 - acc: 0.9912\n",
            "Epoch 96/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0836 - acc: 0.9735\n",
            "Epoch 97/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0848 - acc: 0.9735\n",
            "Epoch 98/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0813 - acc: 0.9823\n",
            "Epoch 99/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0818 - acc: 0.9912\n",
            "Epoch 100/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0852 - acc: 0.9912\n",
            "37/37 [==============================] - 2s 41ms/step\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 4s 38ms/step - loss: 0.8900 - acc: 0.7876\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6150 - acc: 0.7876\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5485 - acc: 0.7965\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5016 - acc: 0.7876\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4559 - acc: 0.7965\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4249 - acc: 0.7965\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4202 - acc: 0.7965\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3840 - acc: 0.8319\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3834 - acc: 0.7965\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3615 - acc: 0.8230\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3545 - acc: 0.8230\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3388 - acc: 0.8407\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3265 - acc: 0.8584\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3267 - acc: 0.9027\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2954 - acc: 0.8496\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3006 - acc: 0.9027\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2859 - acc: 0.9381\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2864 - acc: 0.8850\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2735 - acc: 0.8673\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2718 - acc: 0.9292\n",
            "Epoch 21/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2630 - acc: 0.9381\n",
            "Epoch 22/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2573 - acc: 0.9115\n",
            "Epoch 23/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2456 - acc: 0.9027\n",
            "Epoch 24/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2477 - acc: 0.9204\n",
            "Epoch 25/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2373 - acc: 0.9381\n",
            "Epoch 26/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2302 - acc: 0.9381\n",
            "Epoch 27/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2248 - acc: 0.9292\n",
            "Epoch 28/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2217 - acc: 0.9646\n",
            "Epoch 29/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2156 - acc: 0.9381\n",
            "Epoch 30/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2152 - acc: 0.9558\n",
            "Epoch 31/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2009 - acc: 0.9469\n",
            "Epoch 32/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2044 - acc: 0.9646\n",
            "Epoch 33/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1918 - acc: 0.9558\n",
            "Epoch 34/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1899 - acc: 0.9735\n",
            "Epoch 35/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1861 - acc: 0.9558\n",
            "Epoch 36/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1804 - acc: 0.9558\n",
            "Epoch 37/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1824 - acc: 0.9646\n",
            "Epoch 38/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1738 - acc: 0.9735\n",
            "Epoch 39/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1751 - acc: 0.9646\n",
            "Epoch 40/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1678 - acc: 0.9735\n",
            "Epoch 41/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1600 - acc: 0.9735\n",
            "Epoch 42/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1527 - acc: 0.9735\n",
            "Epoch 43/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1644 - acc: 0.9558\n",
            "Epoch 44/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1506 - acc: 0.9735\n",
            "Epoch 45/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1524 - acc: 0.9469\n",
            "Epoch 46/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1487 - acc: 0.9735\n",
            "Epoch 47/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1444 - acc: 0.9823\n",
            "Epoch 48/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1452 - acc: 0.9558\n",
            "Epoch 49/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1398 - acc: 0.9735\n",
            "Epoch 50/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1357 - acc: 0.9735\n",
            "Epoch 51/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1339 - acc: 0.9558\n",
            "Epoch 52/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1378 - acc: 0.9646\n",
            "Epoch 53/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1313 - acc: 0.9646\n",
            "Epoch 54/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1308 - acc: 0.9646\n",
            "Epoch 55/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1223 - acc: 0.9735\n",
            "Epoch 56/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1220 - acc: 0.9735\n",
            "Epoch 57/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1203 - acc: 0.9735\n",
            "Epoch 58/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1235 - acc: 0.9735\n",
            "Epoch 59/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1158 - acc: 0.9823\n",
            "Epoch 60/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1104 - acc: 0.9735\n",
            "Epoch 61/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1152 - acc: 0.9735\n",
            "Epoch 62/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1155 - acc: 0.9735\n",
            "Epoch 63/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1160 - acc: 0.9646\n",
            "Epoch 64/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.1096 - acc: 0.9735\n",
            "Epoch 65/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1044 - acc: 0.9735\n",
            "Epoch 66/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1161 - acc: 0.9735\n",
            "Epoch 67/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1051 - acc: 0.9823\n",
            "Epoch 68/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1007 - acc: 0.9735\n",
            "Epoch 69/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1004 - acc: 0.9735\n",
            "Epoch 70/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1019 - acc: 0.9823\n",
            "Epoch 71/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1027 - acc: 0.9735\n",
            "Epoch 72/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1001 - acc: 0.9735\n",
            "Epoch 73/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1004 - acc: 0.9735\n",
            "Epoch 74/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0963 - acc: 0.9735\n",
            "Epoch 75/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0938 - acc: 0.9735\n",
            "Epoch 76/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0988 - acc: 0.9823\n",
            "Epoch 77/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0939 - acc: 0.9735\n",
            "Epoch 78/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0869 - acc: 0.9735\n",
            "Epoch 79/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0946 - acc: 0.9646\n",
            "Epoch 80/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0888 - acc: 0.9646\n",
            "Epoch 81/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0864 - acc: 0.9823\n",
            "Epoch 82/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0883 - acc: 0.9823\n",
            "Epoch 83/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0856 - acc: 0.9735\n",
            "Epoch 84/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0864 - acc: 0.9823\n",
            "Epoch 85/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0878 - acc: 0.9735\n",
            "Epoch 86/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0912 - acc: 0.9823\n",
            "Epoch 87/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0776 - acc: 0.9823\n",
            "Epoch 88/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0876 - acc: 0.9823\n",
            "Epoch 89/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0797 - acc: 0.9735\n",
            "Epoch 90/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0795 - acc: 0.9646\n",
            "Epoch 91/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0858 - acc: 0.9646\n",
            "Epoch 92/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0844 - acc: 0.9646\n",
            "Epoch 93/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0740 - acc: 0.9735\n",
            "Epoch 94/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0806 - acc: 0.9735\n",
            "Epoch 95/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0797 - acc: 0.9823\n",
            "Epoch 96/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0773 - acc: 0.9735\n",
            "Epoch 97/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0794 - acc: 0.9823\n",
            "Epoch 98/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0789 - acc: 0.9558\n",
            "Epoch 99/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0807 - acc: 0.9735\n",
            "Epoch 100/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0814 - acc: 0.9735\n",
            "37/37 [==============================] - 2s 43ms/step\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "113/113 [==============================] - 4s 38ms/step - loss: 1.4225 - acc: 0.2124\n",
            "Epoch 2/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.7877 - acc: 0.8761\n",
            "Epoch 3/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.6276 - acc: 0.8850\n",
            "Epoch 4/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.5256 - acc: 0.8850\n",
            "Epoch 5/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4504 - acc: 0.8850\n",
            "Epoch 6/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.4071 - acc: 0.8850\n",
            "Epoch 7/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3724 - acc: 0.8850\n",
            "Epoch 8/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3467 - acc: 0.8850\n",
            "Epoch 9/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.3264 - acc: 0.8850\n",
            "Epoch 10/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.3118 - acc: 0.8850\n",
            "Epoch 11/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2978 - acc: 0.8850\n",
            "Epoch 12/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2877 - acc: 0.8850\n",
            "Epoch 13/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2745 - acc: 0.8850\n",
            "Epoch 14/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2641 - acc: 0.8850\n",
            "Epoch 15/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2575 - acc: 0.8938\n",
            "Epoch 16/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2450 - acc: 0.8938\n",
            "Epoch 17/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2368 - acc: 0.8850\n",
            "Epoch 18/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2268 - acc: 0.8938\n",
            "Epoch 19/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2165 - acc: 0.9027\n",
            "Epoch 20/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2077 - acc: 0.9292\n",
            "Epoch 21/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.2041 - acc: 0.8938\n",
            "Epoch 22/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1951 - acc: 0.9292\n",
            "Epoch 23/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1953 - acc: 0.9027\n",
            "Epoch 24/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1851 - acc: 0.9027\n",
            "Epoch 25/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1763 - acc: 0.9292\n",
            "Epoch 26/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1696 - acc: 0.9469\n",
            "Epoch 27/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1649 - acc: 0.9558\n",
            "Epoch 28/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1595 - acc: 0.9558\n",
            "Epoch 29/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1553 - acc: 0.9469\n",
            "Epoch 30/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1492 - acc: 0.9381\n",
            "Epoch 31/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1427 - acc: 0.9646\n",
            "Epoch 32/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1404 - acc: 0.9558\n",
            "Epoch 33/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1307 - acc: 0.9823\n",
            "Epoch 34/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1299 - acc: 0.9646\n",
            "Epoch 35/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1260 - acc: 0.9735\n",
            "Epoch 36/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1226 - acc: 0.9735\n",
            "Epoch 37/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1158 - acc: 0.9823\n",
            "Epoch 38/100\n",
            "113/113 [==============================] - 1s 6ms/step - loss: 0.1161 - acc: 0.9735\n",
            "Epoch 39/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1127 - acc: 0.9558\n",
            "Epoch 40/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1090 - acc: 0.9912\n",
            "Epoch 41/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1066 - acc: 0.9735\n",
            "Epoch 42/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1031 - acc: 0.9823\n",
            "Epoch 43/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.1027 - acc: 0.9912\n",
            "Epoch 44/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0964 - acc: 0.9823\n",
            "Epoch 45/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0963 - acc: 0.9735\n",
            "Epoch 46/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0935 - acc: 0.9912\n",
            "Epoch 47/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0929 - acc: 0.9912\n",
            "Epoch 48/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0869 - acc: 0.9735\n",
            "Epoch 49/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0892 - acc: 0.9823\n",
            "Epoch 50/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0849 - acc: 0.9912\n",
            "Epoch 51/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0810 - acc: 0.9735\n",
            "Epoch 52/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0800 - acc: 0.9823\n",
            "Epoch 53/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0779 - acc: 0.9735\n",
            "Epoch 54/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0753 - acc: 0.9912\n",
            "Epoch 55/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0754 - acc: 0.9823\n",
            "Epoch 56/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0756 - acc: 0.9823\n",
            "Epoch 57/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0713 - acc: 0.9912\n",
            "Epoch 58/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0723 - acc: 0.9912\n",
            "Epoch 59/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0689 - acc: 0.9912\n",
            "Epoch 60/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0680 - acc: 0.9912\n",
            "Epoch 61/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0672 - acc: 0.9823\n",
            "Epoch 62/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0661 - acc: 0.9912\n",
            "Epoch 63/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0720 - acc: 0.9646\n",
            "Epoch 64/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0626 - acc: 0.9912\n",
            "Epoch 65/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0641 - acc: 0.9912\n",
            "Epoch 66/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0629 - acc: 0.9823\n",
            "Epoch 67/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0598 - acc: 0.9912\n",
            "Epoch 68/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0611 - acc: 0.9823\n",
            "Epoch 69/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0587 - acc: 0.9823\n",
            "Epoch 70/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0579 - acc: 1.0000\n",
            "Epoch 71/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0567 - acc: 0.9912\n",
            "Epoch 72/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0561 - acc: 0.9823\n",
            "Epoch 73/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0546 - acc: 0.9912\n",
            "Epoch 74/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0545 - acc: 0.9823\n",
            "Epoch 75/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0582 - acc: 0.9912\n",
            "Epoch 76/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0520 - acc: 0.9823\n",
            "Epoch 77/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0547 - acc: 0.9912\n",
            "Epoch 78/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0530 - acc: 0.9912\n",
            "Epoch 79/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0547 - acc: 0.9823\n",
            "Epoch 80/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0512 - acc: 0.9823\n",
            "Epoch 81/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0526 - acc: 0.9823\n",
            "Epoch 82/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0495 - acc: 0.9823\n",
            "Epoch 83/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0464 - acc: 0.9912\n",
            "Epoch 84/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0485 - acc: 0.9912\n",
            "Epoch 85/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0488 - acc: 0.9823\n",
            "Epoch 86/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0462 - acc: 0.9823\n",
            "Epoch 87/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0481 - acc: 0.9823\n",
            "Epoch 88/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0472 - acc: 0.9912\n",
            "Epoch 89/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0480 - acc: 0.9823\n",
            "Epoch 90/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0461 - acc: 0.9912\n",
            "Epoch 91/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0447 - acc: 0.9912\n",
            "Epoch 92/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0408 - acc: 0.9912\n",
            "Epoch 93/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0476 - acc: 0.9912\n",
            "Epoch 94/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0444 - acc: 0.9823\n",
            "Epoch 95/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0430 - acc: 0.9823\n",
            "Epoch 96/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0417 - acc: 0.9912\n",
            "Epoch 97/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0441 - acc: 0.9912\n",
            "Epoch 98/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0454 - acc: 0.9823\n",
            "Epoch 99/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0406 - acc: 0.9823\n",
            "Epoch 100/100\n",
            "113/113 [==============================] - 1s 5ms/step - loss: 0.0442 - acc: 0.9912\n",
            "37/37 [==============================] - 2s 44ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWOFDS76ozz_",
        "colab_type": "code",
        "outputId": "13cb6846-3276-46e6-fd87-98c5e33a87c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "all_scores\n",
        "np.mean(all_scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9256756764811439"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgKzCzTrp5JK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}